{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-15T20:09:09.911414Z",
     "start_time": "2025-10-15T20:09:09.646120Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "89bc4b77d6b97f02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T20:09:10.526633Z",
     "start_time": "2025-10-15T20:09:09.914354Z"
    }
   },
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('City_Types.csv')\n",
    "columns_to_scale = list(data.select_dtypes(include='number').columns)\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data['month'] = data['Date'].dt.month\n",
    "data['day'] = data['Date'].dt.day\n",
    "data['weekday'] = data['Date'].dt.weekday\n",
    "data = data.drop(columns=['Date'])\n",
    "y = data['Type']\n",
    "x = data.drop(columns=['Type'])\n",
    "x = pd.get_dummies(x)\n",
    "scaler = StandardScaler()\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
    "x_train[columns_to_scale] = scaler.fit_transform(x_train[columns_to_scale])\n",
    "x_test[columns_to_scale] = scaler.transform(x_test[columns_to_scale])"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "941fc7e4b7ff393c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T20:10:19.530943Z",
     "start_time": "2025-10-15T20:10:19.523575Z"
    }
   },
   "source": [
    "def sigmoid(x):\n",
    "    x = np.clip(x, -500, 500)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return float(x > 0)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / np.sum(e_x)\n",
    "\n",
    "def cross_entropy(pred):\n",
    "    return -np.log(pred + 1e-9)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "72857abfb73b07f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T20:10:20.057625Z",
     "start_time": "2025-10-15T20:10:20.044827Z"
    }
   },
   "source": [
    "class Neuron:\n",
    "\n",
    "    def __init__(self, nin: int, activation: str = 'relu'):\n",
    "        self.activation = activation\n",
    "\n",
    "        if activation == 'relu':\n",
    "            # He initialization for ReLU\n",
    "            limit = np.sqrt(2.0 / nin)\n",
    "        else:\n",
    "            # Xavier initialization for sigmoid/tanh\n",
    "            limit = np.sqrt(6.0 / (nin + 1))\n",
    "\n",
    "        self.weights = np.random.uniform(-limit, limit, nin).astype(np.float64)\n",
    "        self.bias = np.random.normal()\n",
    "\n",
    "        self.grads = np.zeros(nin, dtype=np.float64)\n",
    "        self.gradb = np.float64(0.0)\n",
    "        self.inputs = np.array([], dtype=np.float64)\n",
    "\n",
    "        self.activ = 0\n",
    "        self.delta = 0\n",
    "\n",
    "    def __call__(self, x):\n",
    "        interm = np.dot(self.weights, x) + self.bias\n",
    "        self.inputs = np.array(x, dtype=np.float64)\n",
    "        self.res = interm\n",
    "\n",
    "        if self.activation == 'relu':\n",
    "            res = relu(interm)\n",
    "        else:  # sigmoid\n",
    "            res = sigmoid(interm)\n",
    "\n",
    "        self.activ = res\n",
    "        return res\n",
    "\n",
    "    def activation_derivative(self):\n",
    "        if self.activation == 'relu':\n",
    "            return relu_derivative(self.res)\n",
    "        else:\n",
    "            return sigmoid_derivative(self.activ)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Neuron(Weights:{self.weights}, Bias:{self.bias})'\n",
    "\n",
    "class Layer:\n",
    "\n",
    "    def __init__(self, nin: int, nout: int, activation: str = 'relu'):\n",
    "        self.neurons = [Neuron(nin, activation=activation) for _ in range(nout)]\n",
    "        self.activation = activation\n",
    "\n",
    "    def __call__(self, x):\n",
    "        res = np.array([n(x) for n in self.neurons])\n",
    "        return res if len(res) == 2 else res\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Layer: {[x for x in self.neurons]}'\n",
    "\n",
    "    def parameters(self):\n",
    "        return [x for x in self.neurons]\n",
    "\n",
    "class MLP:\n",
    "\n",
    "    def __init__(self, nin: int, nouts: list, activation: str = 'relu'):\n",
    "        sz = [nin] + nouts\n",
    "        self.nn = [Layer(sz[i], sz[i+1], activation=activation) for i in range(len(nouts))]\n",
    "        self.hidden_activation = activation\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.nn:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'NN: {[x for x in self.nn]}'\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for x in self.nn for p in x.parameters()]\n",
    "\n",
    "    def backprop(self, output: np.ndarray, pred: int):\n",
    "        # Compute gradient for softmax + cross-entropy\n",
    "        # The gradient is: predicted_probs - one_hot_true_label\n",
    "        grad = output.copy()\n",
    "        grad[pred] -= 1  # This is correct IF output contains the softmax probabilities\n",
    "\n",
    "        # Backprop through output layer\n",
    "        last_layer = self.nn[-1]\n",
    "        for i, neuron in enumerate(last_layer.neurons):\n",
    "            delta = grad[i]\n",
    "            neuron.gradb = delta\n",
    "            neuron.grads = delta * neuron.inputs\n",
    "            neuron.delta = delta\n",
    "\n",
    "        # Backprop through hidden layers\n",
    "        for layer_idx in range(len(self.nn) - 2, -1, -1):\n",
    "            layer = self.nn[layer_idx]\n",
    "            layer_n = self.nn[layer_idx + 1]\n",
    "            for i, neuron in enumerate(layer.neurons):\n",
    "                downstream = sum(n.weights[i] * n.delta for n in layer_n.neurons)\n",
    "                neuron.delta = downstream * neuron.activation_derivative()\n",
    "                neuron.grads = neuron.delta * neuron.inputs\n",
    "                neuron.gradb = neuron.delta\n",
    "\n",
    "        # Gradient descent\n",
    "        step = 0.01  # Changed from -0.01\n",
    "        for neuron in self.parameters():\n",
    "            neuron.weights -= step * neuron.grads  # Now we subtract (gradient descent)\n",
    "            neuron.bias -= step * neuron.gradb\n",
    "\n",
    "        # Reset gradients\n",
    "        for neuron in self.parameters():\n",
    "            neuron.grads = np.zeros_like(neuron.grads)\n",
    "            neuron.gradb = 0.0"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "d8af005d23dfad10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T20:10:20.847015Z",
     "start_time": "2025-10-15T20:10:20.834017Z"
    }
   },
   "source": [
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "nin = len(x_train[0])\n",
    "mlp = MLP(nin, [8, 2], activation='relu')"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "6bb236b8d5e0e264",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T20:12:08.525359Z",
     "start_time": "2025-10-15T20:10:21.579686Z"
    }
   },
   "source": [
    "epochs = 50\n",
    "losses = []\n",
    "prediction = ['Industrial', 'Residential']\n",
    "\n",
    "# Before training, reinitialize with smaller weights\n",
    "for neuron in mlp.parameters():\n",
    "    nin = len(neuron.weights)\n",
    "    limit = np.sqrt(1.0 / nin)\n",
    "    neuron.weights = np.random.uniform(-limit, limit, nin).astype(np.float64)\n",
    "    neuron.bias = 0.0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    correct = 0  # Track accuracy\n",
    "\n",
    "    for x, y in zip(x_train, y_train):\n",
    "        out = mlp(x)\n",
    "        out = softmax(out)\n",
    "        loss = cross_entropy(out[prediction.index(y)])\n",
    "        total_loss += loss\n",
    "\n",
    "        # Check if prediction is correct\n",
    "        if np.argmax(out) == prediction.index(y):\n",
    "            correct += 1\n",
    "\n",
    "        mlp.backprop(out, prediction.index(y))\n",
    "\n",
    "    avg_loss = total_loss / len(x_train)\n",
    "    accuracy = correct / len(x_train)\n",
    "    losses.append(avg_loss)\n",
    "\n",
    "    if epoch % 10 == 0:  # Print every 10 epochs\n",
    "        print(f\"Epoch {epoch} | Loss: {avg_loss:.4f} | Accuracy: {accuracy:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 0.0635 | Accuracy: 0.9767\n",
      "Epoch 10 | Loss: 0.0000 | Accuracy: 1.0000\n",
      "Epoch 20 | Loss: 0.0000 | Accuracy: 1.0000\n",
      "Epoch 30 | Loss: 0.0000 | Accuracy: 1.0000\n",
      "Epoch 40 | Loss: 0.0000 | Accuracy: 1.0000\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "76be580b9766822b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T20:12:38.122407Z",
     "start_time": "2025-10-15T20:12:37.779707Z"
    }
   },
   "source": [
    "def evaluate_model(mlp, x_test, y_test, prediction_classes):\n",
    "    \"\"\"\n",
    "    Evaluate the model on test data\n",
    "\n",
    "    Parameters:\n",
    "    - mlp: trained MLP model\n",
    "    - x_test: test features\n",
    "    - y_test: test labels\n",
    "    - prediction_classes: list of class names (e.g., ['Industrial', 'Residential'])\n",
    "\n",
    "    Returns:\n",
    "    - accuracy: test accuracy\n",
    "    - predictions: list of predicted labels\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    predictions = []\n",
    "    test_loss = 0\n",
    "\n",
    "    for x, y in zip(x_test, y_test):\n",
    "        # Forward pass\n",
    "        out = mlp(x)\n",
    "        out = softmax(out)\n",
    "\n",
    "        # Get prediction\n",
    "        pred_idx = np.argmax(out)\n",
    "        pred_label = prediction_classes[pred_idx]\n",
    "        predictions.append(pred_label)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        if pred_label == y:\n",
    "            correct += 1\n",
    "\n",
    "        # Calculate loss\n",
    "        true_idx = prediction_classes.index(y)\n",
    "        test_loss += cross_entropy(out[true_idx])\n",
    "\n",
    "    accuracy = correct / len(x_test)\n",
    "    avg_loss = test_loss / len(x_test)\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"TEST SET RESULTS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Test Accuracy: {accuracy:.4f} ({correct}/{len(x_test)})\")\n",
    "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    from collections import Counter\n",
    "    true_counts = Counter(y_test)\n",
    "    pred_counts = Counter(predictions)\n",
    "\n",
    "    print(\"True distribution:\")\n",
    "    for label, count in true_counts.items():\n",
    "        print(f\"  {label}: {count}\")\n",
    "\n",
    "    print(\"\\nPredicted distribution:\")\n",
    "    for label, count in pred_counts.items():\n",
    "        print(f\"  {label}: {count}\")\n",
    "\n",
    "    # Calculate per-class accuracy\n",
    "    print(\"\\nPer-class accuracy:\")\n",
    "    for class_name in prediction_classes:\n",
    "        class_correct = sum(1 for true, pred in zip(y_test, predictions)\n",
    "                           if true == class_name and pred == class_name)\n",
    "        class_total = sum(1 for true in y_test if true == class_name)\n",
    "        if class_total > 0:\n",
    "            class_acc = class_correct / class_total\n",
    "            print(f\"  {class_name}: {class_acc:.4f} ({class_correct}/{class_total})\")\n",
    "\n",
    "    return accuracy, predictions\n",
    "\n",
    "# Use it like this:\n",
    "accuracy, predictions = evaluate_model(mlp, x_test, y_test, prediction)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TEST SET RESULTS\n",
      "==================================================\n",
      "Test Accuracy: 1.0000 (10541/10541)\n",
      "Test Loss: 0.0000\n",
      "==================================================\n",
      "\n",
      "True distribution:\n",
      "  Residential: 5221\n",
      "  Industrial: 5320\n",
      "\n",
      "Predicted distribution:\n",
      "  Residential: 5221\n",
      "  Industrial: 5320\n",
      "\n",
      "Per-class accuracy:\n",
      "  Industrial: 1.0000 (5320/5320)\n",
      "  Residential: 1.0000 (5221/5221)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4f6417c2c76da23b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
