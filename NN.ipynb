{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-13T19:12:34.499316Z",
     "start_time": "2025-10-13T19:12:34.268056Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "89bc4b77d6b97f02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T19:12:35.065472Z",
     "start_time": "2025-10-13T19:12:34.501992Z"
    }
   },
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('City_Types.csv')\n",
    "columns_to_scale = list(data.select_dtypes(include='number').columns)\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data['month'] = data['Date'].dt.month\n",
    "data['day'] = data['Date'].dt.day\n",
    "data['weekday'] = data['Date'].dt.weekday\n",
    "data = data.drop(columns=['Date'])\n",
    "y = data['Type']\n",
    "x = data.drop(columns=['Type'])\n",
    "x = pd.get_dummies(x)\n",
    "scaler = StandardScaler()\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
    "x_train[columns_to_scale] = scaler.fit_transform(x_train[columns_to_scale])\n",
    "x_test[columns_to_scale] = scaler.transform(x_test[columns_to_scale])\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "941fc7e4b7ff393c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T19:12:35.145990Z",
     "start_time": "2025-10-13T19:12:35.144150Z"
    }
   },
   "source": [
    "def sigmoid(x):\n",
    "    x = np.clip(x, -500, 500)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / np.sum(e_x)\n",
    "\n",
    "def cross_entropy(pred):\n",
    "    return -np.log(pred + 1e-9)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "72857abfb73b07f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T19:12:35.164309Z",
     "start_time": "2025-10-13T19:12:35.159438Z"
    }
   },
   "source": [
    "class Neuron:\n",
    "\n",
    "    def __init__(self, nin: int):\n",
    "        self.weights = np.array([np.random.normal() for _ in range(nin)], dtype=np.float64)\n",
    "        self.bias = np.random.normal()\n",
    "\n",
    "        self.grads = np.zeros(nin, dtype=np.float64)\n",
    "        self.gradb = np.float32(0.0)\n",
    "        self.inputs = np.array([], dtype=np.float64)\n",
    "\n",
    "        self.activ = 0\n",
    "        self.delta = 0\n",
    "\n",
    "    def __call__(self, x):\n",
    "        interm = np.dot(self.weights, x) + self.bias\n",
    "        self.inputs = np.array(x, dtype=np.float64)\n",
    "        self.res = interm\n",
    "        res = sigmoid(interm)\n",
    "        self.activ = res\n",
    "        return res\n",
    "\n",
    "    def __repr__(self):\n",
    "        # return f'Neuron(Weights:{self.weights}, Bias:{self.bias})'\n",
    "        return f'Neuron'\n",
    "\n",
    "    def sigmoid_derivative(self):\n",
    "        return self.activ * (1 - self.activ)\n",
    "\n",
    "\n",
    "class Layer:\n",
    "\n",
    "    def __init__(self, nin: int, nout: int):\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        res = np.array([n(x) for n in self.neurons])\n",
    "        return res if len(res) == 2 else res\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Layer: {[x for x in self.neurons]}'\n",
    "\n",
    "    def parameters(self):\n",
    "        return [x for x in self.neurons]\n",
    "\n",
    "class MLP:\n",
    "\n",
    "    def __init__(self, nin: int, nouts: list):\n",
    "        sz = [nin] + nouts\n",
    "        self.nn = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.nn:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'NN: {[x for x in self.nn]}'\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for x in self.nn for p in x.parameters()]\n",
    "\n",
    "    def backprop(self, output: np.ndarray, pred: int):\n",
    "        # Compute gradient for softmax + cross-entropy\n",
    "        # The gradient is: predicted_probs - one_hot_true_label\n",
    "        grad = output.copy()\n",
    "        grad[pred] -= 1  # This is correct IF output contains the softmax probabilities\n",
    "\n",
    "        # Backprop through output layer\n",
    "        last_layer = self.nn[-1]\n",
    "        for i, neuron in enumerate(last_layer.neurons):\n",
    "            delta = grad[i]\n",
    "            neuron.gradb = delta\n",
    "            neuron.grads = delta * neuron.inputs\n",
    "            neuron.delta = delta\n",
    "\n",
    "        # Backprop through hidden layers\n",
    "        for layer_idx in range(len(self.nn) - 2, -1, -1):\n",
    "            layer = self.nn[layer_idx]\n",
    "            layer_n = self.nn[layer_idx + 1]\n",
    "            for i, neuron in enumerate(layer.neurons):\n",
    "                downstream = sum(n.weights[i] * n.delta for n in layer_n.neurons)\n",
    "                neuron.delta = downstream * neuron.sigmoid_derivative()\n",
    "                neuron.grads = neuron.delta * neuron.inputs\n",
    "                neuron.gradb = neuron.delta\n",
    "\n",
    "        # Gradient descent\n",
    "        step = 0.01  # Changed from -0.01\n",
    "        for neuron in self.parameters():\n",
    "            neuron.weights -= step * neuron.grads  # Now we subtract (gradient descent)\n",
    "            neuron.bias -= step * neuron.gradb\n",
    "\n",
    "        # Reset gradients\n",
    "        for neuron in self.parameters():\n",
    "            neuron.grads = np.zeros_like(neuron.grads)\n",
    "            neuron.gradb = 0.0"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "d8af005d23dfad10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T19:29:39.407683Z",
     "start_time": "2025-10-13T19:29:39.399241Z"
    }
   },
   "source": [
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "nin = len(x_train[0])\n",
    "mlp = MLP(nin, [8, 2])"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "6bb236b8d5e0e264",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T19:32:01.913621Z",
     "start_time": "2025-10-13T19:29:40.427360Z"
    }
   },
   "source": [
    "epochs = 50\n",
    "losses = []\n",
    "prediction = ['Industrial', 'Residential']\n",
    "\n",
    "# Before training, reinitialize with smaller weights\n",
    "for neuron in mlp.parameters():\n",
    "    nin = len(neuron.weights)\n",
    "    limit = np.sqrt(1.0 / nin)\n",
    "    neuron.weights = np.random.uniform(-limit, limit, nin).astype(np.float64)\n",
    "    neuron.bias = 0.0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    correct = 0  # Track accuracy\n",
    "\n",
    "    for x, y in zip(x_train, y_train):\n",
    "        out = mlp(x)\n",
    "        out = softmax(out)\n",
    "        loss = cross_entropy(out[prediction.index(y)])\n",
    "        total_loss += loss\n",
    "\n",
    "        # Check if prediction is correct\n",
    "        if np.argmax(out) == prediction.index(y):\n",
    "            correct += 1\n",
    "\n",
    "        mlp.backprop(out, prediction.index(y))\n",
    "\n",
    "    avg_loss = total_loss / len(x_train)\n",
    "    accuracy = correct / len(x_train)\n",
    "    losses.append(avg_loss)\n",
    "\n",
    "    if epoch % 10 == 0:  # Print every 10 epochs\n",
    "        print(f\"Epoch {epoch} | Loss: {avg_loss:.4f} | Accuracy: {accuracy:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 0.3637 | Accuracy: 0.9492\n",
      "Epoch 10 | Loss: 0.3135 | Accuracy: 0.9998\n",
      "Epoch 20 | Loss: 0.3136 | Accuracy: 0.9996\n",
      "Epoch 30 | Loss: 0.3143 | Accuracy: 0.9990\n",
      "Epoch 40 | Loss: 0.3140 | Accuracy: 0.9993\n",
      "Epoch 50 | Loss: 0.3269 | Accuracy: 0.9864\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[17]\u001B[39m\u001B[32m, line 17\u001B[39m\n\u001B[32m     14\u001B[39m correct = \u001B[32m0\u001B[39m  \u001B[38;5;66;03m# Track accuracy\u001B[39;00m\n\u001B[32m     16\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m x, y \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(x_train, y_train):\n\u001B[32m---> \u001B[39m\u001B[32m17\u001B[39m     out = \u001B[43mmlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     18\u001B[39m     out = softmax(out)\n\u001B[32m     19\u001B[39m     loss = cross_entropy(out[prediction.index(y)])\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 53\u001B[39m, in \u001B[36mMLP.__call__\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m     51\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[32m     52\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.nn:\n\u001B[32m---> \u001B[39m\u001B[32m53\u001B[39m         x = \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     54\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 36\u001B[39m, in \u001B[36mLayer.__call__\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m     35\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[32m---> \u001B[39m\u001B[32m36\u001B[39m     res = np.array([\u001B[43mn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m n \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.neurons])\n\u001B[32m     37\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m res \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(res) == \u001B[32m2\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m res\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 18\u001B[39m, in \u001B[36mNeuron.__call__\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m     16\u001B[39m \u001B[38;5;28mself\u001B[39m.inputs = np.array(x, dtype=np.float64)\n\u001B[32m     17\u001B[39m \u001B[38;5;28mself\u001B[39m.res = interm\n\u001B[32m---> \u001B[39m\u001B[32m18\u001B[39m res = \u001B[43msigmoid\u001B[49m\u001B[43m(\u001B[49m\u001B[43minterm\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     19\u001B[39m \u001B[38;5;28mself\u001B[39m.activ = res\n\u001B[32m     20\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 3\u001B[39m, in \u001B[36msigmoid\u001B[39m\u001B[34m(x)\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34msigmoid\u001B[39m(x):\n\u001B[32m      2\u001B[39m     x = np.clip(x, -\u001B[32m500\u001B[39m, \u001B[32m500\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[32m1\u001B[39m / (\u001B[32m1\u001B[39m + \u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mexp\u001B[49m\u001B[43m(\u001B[49m\u001B[43m-\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T19:26:15.337289Z",
     "start_time": "2025-10-13T19:26:15.331410Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset info:\n",
      "Training samples: 42163\n",
      "Test samples: 10541\n",
      "Number of features: 15\n",
      "\n",
      "Class distribution in training:\n",
      "Residential    21131\n",
      "Industrial     21032\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==================================================\n",
      "SAMPLE PREDICTIONS (first 10):\n",
      "==================================================\n",
      "Sample 0:\n",
      "  True: Residential, Predicted: Residential\n",
      "  Probabilities: Industrial=0.2689, Residential=0.7311\n",
      "  Loss: 0.3133\n",
      "\n",
      "Sample 1:\n",
      "  True: Industrial, Predicted: Industrial\n",
      "  Probabilities: Industrial=0.7311, Residential=0.2689\n",
      "  Loss: 0.3133\n",
      "\n",
      "Sample 2:\n",
      "  True: Industrial, Predicted: Industrial\n",
      "  Probabilities: Industrial=0.7311, Residential=0.2689\n",
      "  Loss: 0.3133\n",
      "\n",
      "Sample 3:\n",
      "  True: Residential, Predicted: Residential\n",
      "  Probabilities: Industrial=0.2689, Residential=0.7311\n",
      "  Loss: 0.3133\n",
      "\n",
      "Sample 4:\n",
      "  True: Residential, Predicted: Residential\n",
      "  Probabilities: Industrial=0.2689, Residential=0.7311\n",
      "  Loss: 0.3133\n",
      "\n",
      "Sample 5:\n",
      "  True: Industrial, Predicted: Industrial\n",
      "  Probabilities: Industrial=0.7311, Residential=0.2689\n",
      "  Loss: 0.3133\n",
      "\n",
      "Sample 6:\n",
      "  True: Residential, Predicted: Residential\n",
      "  Probabilities: Industrial=0.2689, Residential=0.7311\n",
      "  Loss: 0.3133\n",
      "\n",
      "Sample 7:\n",
      "  True: Industrial, Predicted: Industrial\n",
      "  Probabilities: Industrial=0.7311, Residential=0.2689\n",
      "  Loss: 0.3133\n",
      "\n",
      "Sample 8:\n",
      "  True: Industrial, Predicted: Industrial\n",
      "  Probabilities: Industrial=0.7311, Residential=0.2689\n",
      "  Loss: 0.3133\n",
      "\n",
      "Sample 9:\n",
      "  True: Residential, Predicted: Residential\n",
      "  Probabilities: Industrial=0.2689, Residential=0.7311\n",
      "  Loss: 0.3133\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First, check your dataset for data leakage\n",
    "print(\"Dataset info:\")\n",
    "print(f\"Training samples: {len(x_train)}\")\n",
    "print(f\"Test samples: {len(x_test)}\")\n",
    "print(f\"Number of features: {len(x_train[0])}\")\n",
    "print(f\"\\nClass distribution in training:\")\n",
    "print(pd.Series(y_train).value_counts())\n",
    "\n",
    "# Check a few predictions\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAMPLE PREDICTIONS (first 10):\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for i in range(min(10, len(x_train))):\n",
    "    x, y = x_train[i], y_train[i]\n",
    "    out = mlp(x)\n",
    "    out = softmax(out)\n",
    "    pred_label = prediction[np.argmax(out)]\n",
    "    true_idx = prediction.index(y)\n",
    "\n",
    "    print(f\"Sample {i}:\")\n",
    "    print(f\"  True: {y}, Predicted: {pred_label}\")\n",
    "    print(f\"  Probabilities: Industrial={out[0]:.4f}, Residential={out[1]:.4f}\")\n",
    "    print(f\"  Loss: {cross_entropy(out[true_idx]):.4f}\")\n",
    "    print()"
   ],
   "id": "5b74f442bb439474",
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T19:27:00.703882Z",
     "start_time": "2025-10-13T19:27:00.653302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the original data and check\n",
    "data = pd.read_csv('City_Types.csv')\n",
    "print(\"\\nOriginal data columns:\")\n",
    "print(data.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(data.head())\n",
    "\n",
    "# Check if there's any column that perfectly predicts the Type\n",
    "print(\"\\nChecking for perfect correlations with Type...\")\n",
    "for col in data.select_dtypes(include='number').columns:\n",
    "    if col != 'Type':\n",
    "        correlation = data.groupby('Type')[col].mean()\n",
    "        print(f\"{col}:\")\n",
    "        print(correlation)\n",
    "        print()"
   ],
   "id": "daaa48bd46e41ecb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original data columns:\n",
      "['Date', 'City', 'CO', 'NO2', 'SO2', 'O3', 'PM2.5', 'PM10', 'Type']\n",
      "\n",
      "First few rows:\n",
      "                        Date    City     CO   NO2   SO2    O3  PM2.5  PM10  \\\n",
      "0  2024-01-01 00:00:00+00:00  Moscow  208.0  15.9  13.2  44.0    8.6   9.4   \n",
      "1  2024-01-01 01:00:00+00:00  Moscow  207.0  17.4  13.7  44.0    8.6  10.5   \n",
      "2  2024-01-01 02:00:00+00:00  Moscow  217.0  19.0  15.5  43.0   10.4  12.9   \n",
      "3  2024-01-01 03:00:00+00:00  Moscow  231.0  21.0  20.7  36.0   12.3  15.3   \n",
      "4  2024-01-01 04:00:00+00:00  Moscow  263.0  34.5  27.2  27.0   13.6  20.0   \n",
      "\n",
      "         Type  \n",
      "0  Industrial  \n",
      "1  Industrial  \n",
      "2  Industrial  \n",
      "3  Industrial  \n",
      "4  Industrial  \n",
      "\n",
      "Checking for perfect correlations with Type...\n",
      "CO:\n",
      "Type\n",
      "Industrial     795.402474\n",
      "Residential    220.658470\n",
      "Name: CO, dtype: float64\n",
      "\n",
      "NO2:\n",
      "Type\n",
      "Industrial     42.450565\n",
      "Residential    16.782419\n",
      "Name: NO2, dtype: float64\n",
      "\n",
      "SO2:\n",
      "Type\n",
      "Industrial     42.139595\n",
      "Residential     2.634904\n",
      "Name: SO2, dtype: float64\n",
      "\n",
      "O3:\n",
      "Type\n",
      "Industrial     53.696873\n",
      "Residential    53.150463\n",
      "Name: O3, dtype: float64\n",
      "\n",
      "PM2.5:\n",
      "Type\n",
      "Industrial     57.846683\n",
      "Residential     8.015407\n",
      "Name: PM2.5, dtype: float64\n",
      "\n",
      "PM10:\n",
      "Type\n",
      "Industrial     89.746626\n",
      "Residential    11.543333\n",
      "Name: PM10, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "76be580b9766822b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T19:17:49.197915Z",
     "start_time": "2025-10-13T19:17:48.615948Z"
    }
   },
   "source": [
    "def evaluate_model(mlp, x_test, y_test, prediction_classes):\n",
    "    \"\"\"\n",
    "    Evaluate the model on test data\n",
    "\n",
    "    Parameters:\n",
    "    - mlp: trained MLP model\n",
    "    - x_test: test features\n",
    "    - y_test: test labels\n",
    "    - prediction_classes: list of class names (e.g., ['Industrial', 'Residential'])\n",
    "\n",
    "    Returns:\n",
    "    - accuracy: test accuracy\n",
    "    - predictions: list of predicted labels\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    predictions = []\n",
    "    test_loss = 0\n",
    "\n",
    "    for x, y in zip(x_test, y_test):\n",
    "        # Forward pass\n",
    "        out = mlp(x)\n",
    "        out = softmax(out)\n",
    "\n",
    "        # Get prediction\n",
    "        pred_idx = np.argmax(out)\n",
    "        pred_label = prediction_classes[pred_idx]\n",
    "        predictions.append(pred_label)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        if pred_label == y:\n",
    "            correct += 1\n",
    "\n",
    "        # Calculate loss\n",
    "        true_idx = prediction_classes.index(y)\n",
    "        test_loss += cross_entropy(out[true_idx])\n",
    "\n",
    "    accuracy = correct / len(x_test)\n",
    "    avg_loss = test_loss / len(x_test)\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"TEST SET RESULTS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Test Accuracy: {accuracy:.4f} ({correct}/{len(x_test)})\")\n",
    "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    from collections import Counter\n",
    "    true_counts = Counter(y_test)\n",
    "    pred_counts = Counter(predictions)\n",
    "\n",
    "    print(\"True distribution:\")\n",
    "    for label, count in true_counts.items():\n",
    "        print(f\"  {label}: {count}\")\n",
    "\n",
    "    print(\"\\nPredicted distribution:\")\n",
    "    for label, count in pred_counts.items():\n",
    "        print(f\"  {label}: {count}\")\n",
    "\n",
    "    # Calculate per-class accuracy\n",
    "    print(\"\\nPer-class accuracy:\")\n",
    "    for class_name in prediction_classes:\n",
    "        class_correct = sum(1 for true, pred in zip(y_test, predictions)\n",
    "                           if true == class_name and pred == class_name)\n",
    "        class_total = sum(1 for true in y_test if true == class_name)\n",
    "        if class_total > 0:\n",
    "            class_acc = class_correct / class_total\n",
    "            print(f\"  {class_name}: {class_acc:.4f} ({class_correct}/{class_total})\")\n",
    "\n",
    "    return accuracy, predictions\n",
    "\n",
    "# Use it like this:\n",
    "accuracy, predictions = evaluate_model(mlp, x_test, y_test, prediction)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TEST SET RESULTS\n",
      "==================================================\n",
      "Test Accuracy: 1.0000 (10541/10541)\n",
      "Test Loss: 0.3133\n",
      "==================================================\n",
      "\n",
      "True distribution:\n",
      "  Residential: 5221\n",
      "  Industrial: 5320\n",
      "\n",
      "Predicted distribution:\n",
      "  Residential: 5221\n",
      "  Industrial: 5320\n",
      "\n",
      "Per-class accuracy:\n",
      "  Industrial: 1.0000 (5320/5320)\n",
      "  Residential: 1.0000 (5221/5221)\n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
